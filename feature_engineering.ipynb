{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Import libs", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 114, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom project_lib import Project\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import chi2"
        }, 
        {
            "source": "# Data Cleansing [coursera]\nIn some process models Data Cleansing is a separate task, it is closely tied to Feature Creation but also draws findings from the Initial Data Exploration task. The actual data transformations are implemented in the Feature Creation asset deliverable; therefore, Data Cleansing is part of the Feature Creation task in this process model.\n\nWhile tuning machine learning models, this deliverable asset is touched on a regular basis anyway because features need to be transformed to increase model performance. In such iterations, often issues with data are detected and therefore need to be corrected/addressed here as well.\n\nThe following none exhaustive list gives you some guidelines:\n\n- Data types Are data types of columns matching their content? E.g. is age stored as integer and not as string?\n- Ranges Does the value distribution of values in a column make sense? Use stats (e.g. min, max, mean, standard deviation) and visualizations (e.g. box-plot, histogram) for help\n- Emptiness Are all values non-null where mandatory? E.g. client IDs\n- Uniqueness Are duplicates present where undesired? E.g. client IDs\n- Set memberships Are only allowed values chosen for categorical or ordinal fields? E.g. Female, Male, Unknown\n- Foreign key set memberships Are only allowed values chosen as field? E.g. ZIP code\n- Regular expressions Some files need to stick to a pattern expressed by a regular expression. E.g. a lower-case character followed by 6 digits\n- Cross-field validation Some fields can impact validity of other fields. E.g. a male person can\u2019t be pregnant\n", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "# Load data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 93, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "execution_count": 94, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Get the url\ndata_file = project.get_file(\"init_data.csv\")\n# Fetch the CSV file from the object storage using Spark\ndf = pd.read_csv(data_file)"
        }, 
        {
            "source": "# Utile Functions", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 140, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def print_corr(df, attr):\n    lb_make = LabelEncoder()\n    if df[attr].dtype == 'object':\n        src_enc = lb_make.fit_transform(df[attr])\n    else:\n        src_enc = df[attr]\n    \n    target_enc = lb_make.fit_transform(df['target'])\n    print('Corr between', attr, 'and the target: ', np.corrcoef(src_enc, target_enc )[0][1])\n\ndef print_corr_chi(df, attr):\n    lb_make = LabelEncoder()\n    if df[attr].dtype == 'object':\n        src_enc = lb_make.fit_transform(df[attr])\n    else:\n        src_enc = df[attr].values\n    target_enc = lb_make.fit_transform(df['target'])\n    score = chi2(src_enc.reshape(-1, 1), target_enc)[0]\n    print('Corr between', attr, 'and the target: ', score[0])"
        }, 
        {
            "source": "# Remove Duplicated rows", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 96, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Number of duplicates: 29\n"
                }
            ], 
            "source": "print('Number of duplicates:', len(df[df.duplicated()]))"
        }, 
        {
            "source": "We should delete duplicated rows because they don't add new information", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 97, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Number of duplicates: 0\n"
                }
            ], 
            "source": "df = df.drop_duplicates(keep = 'first')\nprint('Number of duplicates:', len(df[df.duplicated()]))"
        }, 
        {
            "source": "# Clean data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Remove extra characters and spaces", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 98, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 98, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": " <=50K    37128\n >50K     11685\nName: target, dtype: int64"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "df['target'] = df['target'].str.replace('.', '')\ndf['target'].value_counts()"
        }, 
        {
            "execution_count": 99, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "for col in df.columns:\n    if df[col].dtype == 'object':\n        df[col] = df[col].str.strip()\n        df[col] = df[col].str.lower()"
        }, 
        {
            "source": "## Cross-field validation\n- a male can not be a wife\n- a female can't be a husband\n- a never married, divorced or widowed can't be husband or wife\n- a separated person can't be wife or husband in the houshold", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 100, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df.loc[(df['sex'] == 'female') & (df['relationship'] == 'husband'), 'relationship'] = 'wife'"
        }, 
        {
            "execution_count": 101, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df.loc[(df['sex'] == 'male') & (df['relationship'] == 'wife'), 'relationship'] = 'husband'"
        }, 
        {
            "execution_count": 102, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 102, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "len(df[(df['marital_status'].isin(['never-married', 'divorced', 'widowed'])) & (df['relationship'].isin(['husband','wife']))])"
        }, 
        {
            "execution_count": 103, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 103, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "len(df[(df['marital_status'].isin(['married-spouse-absent'])) & (df['relationship'].isin(['husband', 'wife']))])"
        }, 
        {
            "source": "# Feature Engineering [coursera]\nFeature Creation and Feature Engineering is one of the most important tasks in machine learning since it hugely impacts model performance. This also holds for deep learning, although to a lesser extent. Features can be changed or new features can be created from existing ones\n\nThe following none exhaustive list gives you some guidelines for feature transformation:\n- Imputing Some algorithms are very sensitive to missing values. Therefore, imputing allows for filling of empty fields based on its value distribution\n- Imputed time-series quantization Time series often contain streams with measurements at different timestamps. Therefore, it is beneficial to quantize measurements to a common \u201cheart beat\u201d and impute the corresponding values. This can be done by sampling from the source time series distributions on the respective quantized time steps\n- Scaling / Normalizing / Centering Some algorithms are very sensitive differences in value ranges for individual fields. Therefore, it is best practice to center data around zero and scale values to a standard deviation of one\n- Filtering Sometimes imputing values doesn\u2019t perform well, therefore deletion of low quality records is a better strategy\n- Discretizing Continuous fields might confuse the model, e.g. a discrete set of age ranges sometimes performs better than continuous values, especially on smaller amounts of data and with simpler models\n\n\nThe following none exhaustive list gives you some guidelines for feature creation:\n- One-hot-encoding Categorical integer features should be transformed into \u201cone-hot\u201d vectors. In relational terms this results in addition of additional columns \u2013 one columns for each distinct category\n- Time-to-Frequency transformation Time-series (and sometimes also sequence data) is recorded in the time domain but can easily transformed into the frequency domain e.g. using FFT (Fast Fourier Transformation)\n- Month-From-Date Creating an additional feature containing the month independent from data captures seasonal aspects. Sometimes further discretization in to quarters helps as well\n- Aggregate-on-Target Simply aggregating fields the target variable (or even other fields) can improve performance, e.g. count number of data points per ZIP code or take the median of all values by geographical region\n\nAs feature engineering is an art on itself, this list cannot be exhaustive. It\u2019s not expected to become an expert in this topic at this point. Most of it you\u2019ll learn by practicing data science on real projects and talk to peers which might share their secrets and tricks with you.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Fill missing values", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 104, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "There is:  0  missing values in  age\nThere is:  2799  missing values in  workclass\nThere is:  0  missing values in  final_weight\nThere is:  0  missing values in  education\nThere is:  0  missing values in  education_duration\nThere is:  0  missing values in  marital_status\nThere is:  2809  missing values in  occupation\nThere is:  0  missing values in  relationship\nThere is:  0  missing values in  race\nThere is:  0  missing values in  sex\nThere is:  0  missing values in  capital_gain\nThere is:  0  missing values in  capital_loss\nThere is:  0  missing values in  hours_per_week\nThere is:  856  missing values in  native_country\nThere is:  0  missing values in  target\n"
                }
            ], 
            "source": "for col in df.columns:\n    print('There is: ', len(df[df[col] == '?']), ' missing values in ', col)"
        }, 
        {
            "execution_count": 105, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "for col in df.columns:\n    df.loc[df[col] == '?', col] = 'other'"
        }, 
        {
            "source": "## Add missing values feature\nThis feature indicates how missing values each row has", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 106, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df['has_missing_values'] = 0\ndef detect_missing_value(row):\n    for col in df.columns:\n        if row[col] == 'other':\n            row['has_missing_values'] += 1\n    return row\ndf = df.apply(detect_missing_value, axis = 1)"
        }, 
        {
            "execution_count": 107, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 107, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0    44841\n2     2746\n1     1151\n3       73\n4        2\nName: has_missing_values, dtype: int64"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "df['has_missing_values'].value_counts()"
        }, 
        {
            "execution_count": 135, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Corr between has_missing_values and the target:  570.2245452953478\n"
                }
            ], 
            "source": "print_corr_chi(df, 'has_missing_values')"
        }, 
        {
            "source": "## Combine categorical levels", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### workclass\n\n- federal-gov, local_gov, state_gov -> government\n- private -> private\n- self-emp-inc, self-emp-not-inc -> self-employed\n- never-worked, without-pay -> unemployed\n- ? -> other", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 138, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df['com_workclass'] = df['workclass']\ndf.loc[df['com_workclass'].isin(['state-gov', 'federal-gov', 'local-gov']), 'com_workclass'] = 'government'\ndf.loc[df['com_workclass'].isin(['self-emp-inc', 'self-emp-not-inc']), 'com_workclass'] = 'self-employed'\ndf.loc[df['com_workclass'].isin(['without-pay', 'never-worked']), 'com_workclass'] = 'unemployed'"
        }, 
        {
            "execution_count": 141, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Corr between workclass and the target:  5.0402826956667e-05\nCorr between com_workclass and the target:  2.011445743802379\n"
                }
            ], 
            "source": "print_corr_chi(df, 'workclass')\nprint_corr_chi(df, 'com_workclass')"
        }, 
        {
            "source": "# marital_status", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": " - \u201cnever-married,\u201d \u201cwidowed,\u201d and \u201cdivorced\u201d ->  \u201csingle\u201d\n - \u201cmarried-civ-spouse\u201d, \u201cseparated\u201d, \u201cmarried-af-spouse\u201d and \u201cmarried-spouse-absent\u201d -> \u201cmarried\u201d", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 142, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df['com_marital_status'] = df['marital_status']\ndf.loc[df['com_marital_status'].isin(['never-married' 'widowed', 'divorced']), 'com_marital_status'] = 'single'\ndf.loc[df['com_marital_status'].isin(\n    ['married-civ-spouse' 'separated', 'married-af-spouse', 'married-spouse-absent']), 'com_marital_status'] = 'single'"
        }, 
        {
            "execution_count": 144, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Corr between marital_status and the target:  1678.3467045745847\nCorr between com_marital_status and the target:  6744.572692797321\n"
                }
            ], 
            "source": "print_corr_chi(df, 'marital_status')\nprint_corr_chi(df, 'com_marital_status')"
        }, 
        {
            "source": "# native_country", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 158, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "north_america = ['united-states', 'cuba', 'jamaica', 'mexico', 'canada',\n'puerto-rico', 'honduras',  'haiti', 'dominican-republic', \n'el-salvador',  'guatemala',  'outlying-us(guam-usvi-etc)',  \n'trinadad&tobago', 'nicaragua',  \n]\n\nother = ['other', 'south']\n\nasia = ['india', 'china', 'japan', 'vietnam', 'hong', 'iran', 'philippines', 'cambodia', 'thailand', 'laos', 'taiwan']\n\neurope = ['england', 'germany', 'italy', 'poland', 'ireland', 'hungary',\n'holand-netherlands', 'scotland', 'yugoslavia', 'greece', 'france', 'portugal',]\n\nsouth_america = ['peru',  'columbia',  'ecuador', ]"
        }, 
        {
            "execution_count": 159, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 159, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "42"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "df.loc[df['native_country'].isin(asia), 'com_native_country'] = 'aisa'\ndf.loc[df['native_country'].isin(europe), 'com_native_country'] = 'europe'\ndf.loc[df['native_country'].isin(north_america), 'com_native_country'] = 'no'\ndf.loc[df['native_country'].isin(asia), 'com_native_country'] = 'aisa'"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.6.8", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}