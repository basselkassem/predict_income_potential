{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Extract Transform Load (ETL) [coursera]\n\nETL is one of the first things which needs to be done in a data science project. \n\nThe nature of this task highly depends on the type of data source. Whether it is relational or unstructured, enterprise data or internet data, persistent data or streaming data. This heavily influences the choice of architecture. Therefore, you must document your choice and thinking process in the Architectural Decision Document (ADD).\n\nThis task involves \u2013 as the name implies \u2013 accessing the data source, transforming it in a way it can be easily worked with and finally make it available to downstream analytics processes \u2013 either real-time streaming or batch ones.\n\n- In case of operational relational data, de-normalization usually needs to take place\n- for unstructured data, some feature extraction might already be appropriate\n- for real-time data, windows are usually created.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "# Downloading data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "--2019-10-13 10:26:42--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\nResolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\nConnecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3974305 (3.8M) [application/x-httpd-php]\nSaving to: \u2018adult.data.1\u2019\n\n100%[======================================>] 3,974,305   12.5MB/s   in 0.3s   \n\n2019-10-13 10:26:43 (12.5 MB/s) - \u2018adult.data.1\u2019 saved [3974305/3974305]\n\n"
                }
            ], 
            "source": "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "--2019-10-13 10:26:44--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\nResolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\nConnecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2003153 (1.9M) [application/x-httpd-php]\nSaving to: \u2018adult.test.1\u2019\n\n100%[======================================>] 2,003,153   8.22MB/s   in 0.2s   \n\n2019-10-13 10:26:44 (8.22 MB/s) - \u2018adult.test.1\u2019 saved [2003153/2003153]\n\n"
                }
            ], 
            "source": "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test"
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "--2019-10-13 10:26:45--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names\nResolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\nConnecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 5229 (5.1K) [application/x-httpd-php]\nSaving to: \u2018adult.names.1\u2019\n\n100%[======================================>] 5,229       --.-K/s   in 0s      \n\n2019-10-13 10:26:45 (69.1 MB/s) - \u2018adult.names.1\u2019 saved [5229/5229]\n\n"
                }
            ], 
            "source": "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names"
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "adult.data  adult.data.1  adult.names  adult.names.1  adult.test  adult.test.1\r\n"
                }
            ], 
            "source": "!ls"
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "| This data was extracted from the census bureau database found at\r\n| http://www.census.gov/ftp/pub/DES/www/welcome.html\r\n| Donor: Ronny Kohavi and Barry Becker,\r\n|        Data Mining and Visualization\r\n|        Silicon Graphics.\r\n|        e-mail: ronnyk@sgi.com for questions.\r\n| Split into train-test using MLC++ GenCVFiles (2/3, 1/3 random).\r\n| 48842 instances, mix of continuous and discrete    (train=32561, test=16281)\r\n| 45222 if instances with unknown values are removed (train=30162, test=15060)\r\n| Duplicate or conflicting instances : 6\r\n| Class probabilities for adult.all file\r\n| Probability for the label '>50K'  : 23.93% / 24.78% (without unknowns)\r\n| Probability for the label '<=50K' : 76.07% / 75.22% (without unknowns)\r\n|\r\n| Extraction was done by Barry Becker from the 1994 Census database.  A set of\r\n|   reasonably clean records was extracted using the following conditions:\r\n|   ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\r\n|\r\n| Prediction task is to determine whether a person makes over 50K\r\n| a year.\r\n|\r\n| First cited in:\r\n| @inproceedings{kohavi-nbtree,\r\n|    author={Ron Kohavi},\r\n|    title={Scaling Up the Accuracy of Naive-Bayes Classifiers: a\r\n|           Decision-Tree Hybrid},\r\n|    booktitle={Proceedings of the Second International Conference on\r\n|               Knowledge Discovery and Data Mining},\r\n|    year = 1996,\r\n|    pages={to appear}}\r\n|\r\n| Error Accuracy reported as follows, after removal of unknowns from\r\n|    train/test sets):\r\n|    C4.5       : 84.46+-0.30\r\n|    Naive-Bayes: 83.88+-0.30\r\n|    NBTree     : 85.90+-0.28\r\n|\r\n|\r\n| Following algorithms were later run with the following error rates,\r\n|    all after removal of unknowns and using the original train/test split.\r\n|    All these numbers are straight runs using MLC++ with default values.\r\n|\r\n|    Algorithm               Error\r\n| -- ----------------        -----\r\n| 1  C4.5                    15.54\r\n| 2  C4.5-auto               14.46\r\n| 3  C4.5 rules              14.94\r\n| 4  Voted ID3 (0.6)         15.64\r\n| 5  Voted ID3 (0.8)         16.47\r\n| 6  T2                      16.84\r\n| 7  1R                      19.54\r\n| 8  NBTree                  14.10\r\n| 9  CN2                     16.00\r\n| 10 HOODG                   14.82\r\n| 11 FSS Naive Bayes         14.05\r\n| 12 IDTM (Decision table)   14.46\r\n| 13 Naive-Bayes             16.12\r\n| 14 Nearest-neighbor (1)    21.42\r\n| 15 Nearest-neighbor (3)    20.35\r\n| 16 OC1                     15.04\r\n| 17 Pebls                   Crashed.  Unknown why (bounds WERE increased)\r\n|\r\n| Conversion of original data as follows:\r\n| 1. Discretized agrossincome into two ranges with threshold 50,000.\r\n| 2. Convert U.S. to US to avoid periods.\r\n| 3. Convert Unknown to \"?\"\r\n| 4. Run MLC++ GenCVFiles to generate data,test.\r\n|\r\n| Description of fnlwgt (final weight)\r\n|\r\n| The weights on the CPS files are controlled to independent estimates of the\r\n| civilian noninstitutional population of the US.  These are prepared monthly\r\n| for us by Population Division here at the Census Bureau.  We use 3 sets of\r\n| controls.\r\n|  These are:\r\n|          1.  A single cell estimate of the population 16+ for each state.\r\n|          2.  Controls for Hispanic Origin by age and sex.\r\n|          3.  Controls by Race, age and sex.\r\n|\r\n| We use all three sets of controls in our weighting program and \"rake\" through\r\n| them 6 times so that by the end we come back to all the controls we used.\r\n|\r\n| The term estimate refers to population totals derived from CPS by creating\r\n| \"weighted tallies\" of any specified socio-economic characteristics of the\r\n| population.\r\n|\r\n| People with similar demographic characteristics should have\r\n| similar weights.  There is one important caveat to remember\r\n| about this statement.  That is that since the CPS sample is\r\n| actually a collection of 51 state samples, each with its own\r\n| probability of selection, the statement only applies within\r\n| state.\r\n\r\n\r\n>50K, <=50K.\r\n\r\nage: continuous.\r\nworkclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\r\nfnlwgt: continuous.\r\neducation: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\r\neducation-num: continuous.\r\nmarital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\r\noccupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\r\nrelationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\r\nrace: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\r\nsex: Female, Male.\r\ncapital-gain: continuous.\r\ncapital-loss: continuous.\r\nhours-per-week: continuous.\r\nnative-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\r\n"
                }
            ], 
            "source": "!cat adult.names"
        }, 
        {
            "source": "# Importing libs", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import pandas as pd\nfrom project_lib import Project"
        }, 
        {
            "source": "# Reading Data\n\n\nThe downloaded data is a tabular data that exists in tow files with comma separated format. We are going to:\n- Assign names to the columns\n- Read data form the 2 files and merge them\n- Save the merged data to IBM object storage\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 23, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Data attributes:  15\n"
                }
            ], 
            "source": "data_headers = ['age', 'workclass', 'final_weight', 'education', 'education_duration', \n                'marital_status', 'occupation', 'relationship', 'race', 'sex', \n                'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'target']\n\nprint('Data attributes: ', len(data_headers))"
        }, 
        {
            "execution_count": 24, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "part1 = pd.read_csv('adult.data', header = None, names = data_headers)\npart2 = pd.read_csv('adult.test', header = None, skiprows = 1, names = data_headers)"
        }, 
        {
            "execution_count": 25, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "First part of the data:  (32561, 15)\nSecond part of the data:  (16281, 15)\ntotal size of the data: (48842, 15)\n"
                }
            ], 
            "source": "print('First part of the data: ', part1.shape)\nprint('Second part of the data: ', part2.shape)\ndf = pd.concat([part1, part2], ignore_index = True, axis = 0)\nprint('total size of the data:', df.shape)"
        }, 
        {
            "execution_count": 26, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 26, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>workclass</th>\n      <th>final_weight</th>\n      <th>education</th>\n      <th>education_duration</th>\n      <th>marital_status</th>\n      <th>occupation</th>\n      <th>relationship</th>\n      <th>race</th>\n      <th>sex</th>\n      <th>capital_gain</th>\n      <th>capital_loss</th>\n      <th>hours_per_week</th>\n      <th>native_country</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>39</td>\n      <td>State-gov</td>\n      <td>77516</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Never-married</td>\n      <td>Adm-clerical</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>2174</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>50</td>\n      <td>Self-emp-not-inc</td>\n      <td>83311</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Exec-managerial</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>38</td>\n      <td>Private</td>\n      <td>215646</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Divorced</td>\n      <td>Handlers-cleaners</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>53</td>\n      <td>Private</td>\n      <td>234721</td>\n      <td>11th</td>\n      <td>7</td>\n      <td>Married-civ-spouse</td>\n      <td>Handlers-cleaners</td>\n      <td>Husband</td>\n      <td>Black</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28</td>\n      <td>Private</td>\n      <td>338409</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Prof-specialty</td>\n      <td>Wife</td>\n      <td>Black</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>Cuba</td>\n      <td>&lt;=50K</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "   age          workclass  final_weight   education  education_duration  \\\n0   39          State-gov         77516   Bachelors                  13   \n1   50   Self-emp-not-inc         83311   Bachelors                  13   \n2   38            Private        215646     HS-grad                   9   \n3   53            Private        234721        11th                   7   \n4   28            Private        338409   Bachelors                  13   \n\n        marital_status          occupation    relationship    race      sex  \\\n0        Never-married        Adm-clerical   Not-in-family   White     Male   \n1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n\n   capital_gain  capital_loss  hours_per_week  native_country  target  \n0          2174             0              40   United-States   <=50K  \n1             0             0              13   United-States   <=50K  \n2             0             0              40   United-States   <=50K  \n3             0             0              40   United-States   <=50K  \n4             0             0              40            Cuba   <=50K  "
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "df.head()"
        }, 
        {
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "execution_count": 29, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 29, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "{'file_name': 'init_data.csv',\n 'message': 'File saved to project storage.',\n 'bucket_name': 'predictincomepotential-donotdelete-pr-tg55qtqcmgw2hz',\n 'asset_id': 'c7aa7f2d-756f-49ce-b3cf-8cfb1de2d8bd'}"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "project.save_data(\"init_data.csv\", df.to_csv(index = 0))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.6.8", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}